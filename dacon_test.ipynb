{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv', encoding = 'utf-8-sig')\n",
    "test = pd.read_csv('./test.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    sample = f\"input : {train['input'][i]} \\n output : {train['output'][i]}\"\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = 'beomi/gemma-ko-7b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer \n",
    ")\n",
    "\n",
    "restored_reviews = []\n",
    "\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    query = row['input']  \n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "                \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "                \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "                \"Below are examples of obfuscated Korean reviews and their restored forms:\\n\\n\"\n",
    "                f\"Example, {samples}\"  \n",
    "                \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "                \"Do not provide any description. Print only in Korean.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"input : {query}, output : \"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt = \"\\n\".join([m[\"content\"] for m in messages]).strip()\n",
    "    \n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=len(query),\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = outputs[0]['generated_text']\n",
    "    result = generated_text[len(prompt):].strip()\n",
    "        \n",
    "\n",
    "    restored_reviews.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965\n"
     ]
    }
   ],
   "source": [
    "rc=pd.read_csv('test.csv')\n",
    "\n",
    "max=0\n",
    "\n",
    "for i, row in rc.iterrows():\n",
    "    if len(row['input'])>max:\n",
    "        max=len(row['input'])\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import time\n",
    "\n",
    "# 샘플 데이터셋 생성 (50개 샘플)\n",
    "sample_data = {\n",
    "    \"input\": [\n",
    "        \"This is a test input for checking the tokenization process.\",\n",
    "        \"Another sample input, short and simple.\",\n",
    "        \"Here is a longer sentence that will require more tokens for the model to process.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Sample input with punctuation! How will the tokenizer handle it?\",\n",
    "        \"This is a test sentence to verify batch processing behavior.\",\n",
    "        \"Longer input designed to check if padding or truncation works as expected when used in tokenization.\",\n",
    "        \"Data augmentation helps improve model performance.\",\n",
    "        \"Test sentence to ensure proper padding and truncation works.\",\n",
    "        \"Short input to check the efficiency of tokenization.\",\n",
    "        \"This example is used for testing whether truncation occurs when tokens exceed the max length.\",\n",
    "        \"Random sentence to observe how model processes the input efficiently.\",\n",
    "        \"Text tokenization is an essential process for NLP models.\",\n",
    "        \"Padding tokens are added to match the sequence length.\",\n",
    "        \"This is a simple test to check tokenizer behavior with long texts.\",\n",
    "        \"Tokenizers need to handle punctuation marks carefully.\",\n",
    "        \"Using padding and truncation strategies effectively ensures model stability.\",\n",
    "        \"Input with several sentences to test batch processing capacity.\",\n",
    "        \"Short sentences are often used in fast model evaluation.\",\n",
    "        \"Longer inputs with multi-token words help check model handling capacity.\",\n",
    "        \"Sentences of varied length improve tokenizers' ability to generalize.\",\n",
    "        \"Tokenizer efficiency is crucial for large-scale training.\",\n",
    "        \"Model needs sufficient data to generalize across varied inputs.\",\n",
    "        \"Using multiple samples in training aids model robustness.\",\n",
    "        \"Test input with various punctuation to see if tokenizer handles it correctly.\",\n",
    "        \"This input sentence is designed to be very long and check for truncation.\",\n",
    "        \"This sentence contains multiple clauses to check for tokenization errors.\",\n",
    "        \"Longer sentences should trigger proper truncation behavior in models.\",\n",
    "        \"Short inputs require minimal padding to fit within token limits.\",\n",
    "        \"Medium-length sentences are ideal for checking padding behavior.\",\n",
    "        \"Using long sentences in NLP models ensures the tokenizer works efficiently.\",\n",
    "        \"Balanced sentences help improve model performance during training.\",\n",
    "        \"Test sentences of different lengths help improve batch processing capabilities.\",\n",
    "        \"Padding and truncation methods help ensure model accuracy on various inputs.\",\n",
    "        \"Truncating long sentences ensures the model doesn't exceed memory limits.\",\n",
    "        \"Shorter inputs ensure the model is capable of handling large batches.\",\n",
    "        \"The sentence length affects both memory consumption and model accuracy.\",\n",
    "        \"Using a mix of sentence lengths improves model training efficiency.\",\n",
    "        \"Appropriate padding and truncation methods help enhance the model's effectiveness.\",\n",
    "        \"Correct tokenization is vital for any NLP model's functionality.\",\n",
    "        \"Batch processing enables efficient handling of large datasets.\",\n",
    "        \"Optimized tokenization leads to faster training times in models.\",\n",
    "        \"Advanced models require efficient tokenizers to process large-scale data.\",\n",
    "        \"Tokenization must be precise to avoid loss of important information.\",\n",
    "        \"Sentence splitting is a crucial process for preparing input data for models.\",\n",
    "        \"For long inputs, truncation is essential to prevent memory overflow.\",\n",
    "        \"Efficient padding helps reduce unnecessary computation during model training.\",\n",
    "        \"Effective truncation ensures the model only processes relevant parts of the input.\"\n",
    "    ],\n",
    "    \"output\": [\n",
    "        \"This is a test output for the first input.\",\n",
    "        \"Output for the second input here.\",\n",
    "        \"The model should process the longer sentence with more tokens.\",\n",
    "        \"The fox jumped over the dog, according to the story.\",\n",
    "        \"Processed output from a sentence with punctuation.\",\n",
    "        \"A test output that matches the input sentence length.\",\n",
    "        \"Testing longer input with padding and truncation in mind.\",\n",
    "        \"Data augmentation improves performance by diversifying inputs.\",\n",
    "        \"Output for a sentence that tests padding and truncation strategies.\",\n",
    "        \"A quick test for short input processing.\",\n",
    "        \"A test for truncation when input tokens exceed max length.\",\n",
    "        \"Random output generated to observe model behavior.\",\n",
    "        \"Output related to tokenization process in NLP models.\",\n",
    "        \"Padding tokens ensure sequences are aligned for model processing.\",\n",
    "        \"Output matches the input sentence's token length after processing.\",\n",
    "        \"Efficient tokenization ensures faster training and fewer errors.\",\n",
    "        \"Longer sentences are processed correctly using proper tokenization.\",\n",
    "        \"Padding and truncation strategies are confirmed to work efficiently.\",\n",
    "        \"Batch processing in NLP tasks enhances speed and stability.\",\n",
    "        \"Output verification for smaller input sentences.\",\n",
    "        \"Longer inputs generate more tokens and require careful tokenization.\",\n",
    "        \"Generalization tests are easier with varied sentence lengths.\",\n",
    "        \"Improved tokenizer efficiency ensures stability in large-scale tasks.\",\n",
    "        \"Model stability is confirmed through multiple input variations.\",\n",
    "        \"Tokenizer handles input variations efficiently for robust model behavior.\",\n",
    "        \"Appropriate sentence structures improve model generalization.\",\n",
    "        \"The truncation of long sentences ensures accuracy without exceeding memory limits.\",\n",
    "        \"Multiple clauses test the tokenizer's handling capacity in NLP tasks.\",\n",
    "        \"Tokenization works as expected for both long and short sentences.\",\n",
    "        \"Short sentences optimize model performance during batch processing.\",\n",
    "        \"Efficient tokenization ensures minimal resource usage during training.\",\n",
    "        \"Balancing input lengths optimizes the model's capacity to generalize.\",\n",
    "        \"Generalization during training is enhanced by varying sentence lengths.\",\n",
    "        \"Padding and truncation support effective model accuracy for various tasks.\",\n",
    "        \"Proper tokenization ensures minimal loss of information during training.\",\n",
    "        \"Batch processing helps models handle large datasets quickly and accurately.\",\n",
    "        \"Optimized tokenization techniques support faster and more accurate model training.\",\n",
    "        \"Truncation is essential for processing large inputs without overloading memory.\",\n",
    "        \"Truncating long sentences maintains memory efficiency while ensuring accuracy.\",\n",
    "        \"Shorter inputs ensure efficient computation during large batch training.\",\n",
    "        \"Model accuracy is not compromised by varying input lengths during training.\",\n",
    "        \"Training efficiency is improved by managing sentence lengths appropriately.\",\n",
    "        \"Optimized truncation ensures that only relevant information is processed.\",\n",
    "        \"Efficient tokenization and truncation methods enhance the model's effectiveness.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# DataFrame 생성 후 Datasets 형식으로 변환\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "dataset = Dataset.from_pandas(sample_df)\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizer 적용\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 배치 내 개별 길이 대응 함수\n",
    "def get_max_new_tokens(batch):\n",
    "    max_input_length = max([len(tokenizer(x)[\"input_ids\"]) for x in batch[\"input\"]])\n",
    "    print(f\"Max input length in this batch: {max_input_length}\")  # 출력 추가\n",
    "    return max_input_length\n",
    "\n",
    "# Trainer 콜백을 사용하여 배치 내 길이 기반 `max_new_tokens` 동적 조정\n",
    "class DynamicMaxTokensCallback(TrainerCallback):\n",
    "    def on_batch_begin(self, args, state, control, **kwargs):\n",
    "        max_new_tokens = get_max_new_tokens(kwargs['inputs'])  # 배치 내 가장 긴 길이 추출\n",
    "        control.max_new_tokens = max_new_tokens\n",
    "        print(f\"Dynamic max_new_tokens: {max_new_tokens}\")  # 출력 추가\n",
    "        return control\n",
    "\n",
    "# TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,  # 기본 배치 사이즈\n",
    "    gradient_accumulation_steps=8,  # Effective batch size\n",
    "    fp16=True,  # FP16 사용\n",
    "    save_steps=1000,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,  # Gradient Checkpointing 활성화\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[DynamicMaxTokensCallback()]  # 콜백으로 동적 `max_new_tokens` 적용\n",
    ")\n",
    "\n",
    "# 실험 시작 전 시간 측정\n",
    "start_time = time.time()\n",
    "\n",
    "# 학습 시작 (샘플 데이터로 학습)\n",
    "trainer.train()\n",
    "\n",
    "# 학습 시간 측정\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time for sample dataset: {elapsed_time / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konoise import NoiseGenerator\n",
    "\n",
    "# train.csv 파일 불러오기\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# NoiseGenerator 객체 초기화 (konoise)\n",
    "generator = NoiseGenerator()\n",
    "\n",
    "# 증강된 데이터 저장할 리스트\n",
    "augmented_data = []\n",
    "\n",
    "# 각 데이터에 대해 증강\n",
    "for idx, row in train_df.iterrows():\n",
    "    input_text = row['input']\n",
    "    output_text = row['output']\n",
    "    \n",
    "    # input 텍스트에 대해 노이즈 추가 (난독화된 텍스트 생성)\n",
    "    augmented_input_list = generator.generate(input_text, methods='disattach-letters', prob=1.0)\n",
    "    \n",
    "    # 리스트에서 첫 번째 항목을 가져와서 슬라이싱\n",
    "    augmented_input = augmented_input_list[0][0]\n",
    "    \n",
    "    # output 텍스트는 그대로 교정된 상태로 유지\n",
    "    augmented_output = output_text.strip()  # 교정된 텍스트\n",
    "    \n",
    "    # 증강된 데이터를 리스트에 저장\n",
    "    augmented_data.append([augmented_input, augmented_output])\n",
    "\n",
    "# 증강된 데이터를 새로운 DataFrame에 저장\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['input', 'output'])\n",
    "\n",
    "# 증강된 데이터만 저장한 CSV 파일\n",
    "augmented_df.to_csv(\"augmented_data.csv\", index=False)\n",
    "\n",
    "# 기존 데이터와 증강된 데이터를 합친 CSV 파일\n",
    "combined_df = pd.concat([train_df[['input', 'output']], augmented_df], ignore_index=True)\n",
    "combined_df.to_csv(\"combined_data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlybooks-llm-Yd1huzAD-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
