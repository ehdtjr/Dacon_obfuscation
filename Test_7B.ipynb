{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqxUJKSUdetP"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U bitsandbytes==0.39.1\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.1\n",
    "!pip3 install -q -U huggingface_hub==0.23.0\n",
    "!pip3 install -q -U triton==2.0.0\n",
    "!pip3 install -q -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPa5j8QLdikV"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoY3EY-ddim5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "Hugging_Access_Token = os.getenv(\"Hugging_Access_Token\")\n",
    "\n",
    "login(token=Hugging_Access_Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_mdCv6mdiqx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79ZFnwzTditd"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./combined_data.csv\", encoding=\"utf-8-sig\")\n",
    "test = pd.read_csv(\"./test.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKhnc4MEdivw"
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    sample = f\"input : {train['input'][i]} \\n output : {train['output'][i]}\"\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP-Lxx43diyh"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC5MGeT2di07"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GemmaTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "model_id = \"beomi/KoAlpaca-KoRWKV-6B\"\n",
    "device = \"auto\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 8비트 양자화로 모델 로드 (BitsAndBytesConfig 없이 load_in_8bit=True 옵션 사용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, load_in_8bit=True, device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvzv9KYodi3q"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOpIcX7Adi6W"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # 모델 입력 토큰화\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "    # causal LM 학습을 위해 labels를 input_ids로 그대로 복사\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# map()을 통해 데이터셋에 토크나이저 적용\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n",
    "\n",
    "\n",
    "class DynamicMaxTokensCallback(TrainerCallback):\n",
    "    def on_batch_begin(self, args, state, control, **kwargs):\n",
    "        # 배치마다 가장 긴 입력 길이를 찾아 동적으로 max_new_tokens를 설정\n",
    "        max_input_length = max(\n",
    "            [len(tokenizer(x)[\"input_ids\"]) for x in kwargs[\"inputs\"][\"input\"]]\n",
    "        )\n",
    "        control.max_new_tokens = max_input_length\n",
    "        print(f\"Dynamic max_new_tokens: {max_input_length}\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxPOKLUUdi80"
   },
   "outputs": [],
   "source": [
    "# # LoRA config\n",
    "# lora_r = 16 #lora 가운데 차원\n",
    "# lora_alpha = 16 #lora 스케일링 alpha/r\n",
    "# lora_dropout = 0.05\n",
    "# lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# LoRA config\n",
    "lora_r = 16  # lora 가운데 차원\n",
    "lora_alpha = 16  # lora 스케일링 alpha/r\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"key\", \"value\", \"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p35AfgF4di_Q"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "# LoRA옵션값 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    # LoRA를 붙이는 위치로, attention쪽, MLP쪽 등 내가 원하는 곳에 붙일수 있다\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 위에서 4bit로 양자한 모델을 준비\n",
    "# 모델을 LoRA붙일수 있게 셋팅\n",
    "model = prepare_model_for_int8_training(model)\n",
    "print(model)\n",
    "# LoRA붙이기\n",
    "model = get_peft_model(model, lora_config)  # Applying LoRA\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_tvVr8zdjBs"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=2,   # 각 GPU/CPU에서 학습할 배치 크기\n",
    "#     gradient_accumulation_steps=8,   # 실제 batch size = 2 * 8 = 16\n",
    "#     fp16=False,\n",
    "#     max_grad_norm=0.0,\n",
    "#     save_steps=1000,\n",
    "#     logging_steps=50,\n",
    "#     evaluation_strategy=\"no\",       # 예시에서는 검증 생략\n",
    "#     num_train_epochs=3,\n",
    "#     save_total_limit=2,\n",
    "#     gradient_checkpointing=True,    # 메모리 절약\n",
    "#     learning_rate=2e-5,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,  # 배치 크기를 16으로 증가 (GPU 메모리가 여유 있을 경우)\n",
    "    gradient_accumulation_steps=1,  # gradient accumulation 단계 1로, 즉 매 배치마다 업데이트\n",
    "    bf16=True,  # A100에서 BF16 사용\n",
    "    max_grad_norm=0.0,  # gradient clipping 비활성화\n",
    "    save_steps=1000,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"no\",\n",
    "    num_train_epochs=1,  # 에폭 수도 1로 줄여서 빠른 프로토타입 진행\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=False,  # 체크포인팅 끄면 속도 향상\n",
    "    learning_rate=2e-5,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    optim=\"adamw_bnb_8bit\",  # 8-bit 옵티마이저 사용 (속도 및 메모리 이점)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P1V-qVfdsvN"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKkQJw2zds0O"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time for sample dataset: {elapsed_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkUiKW28ds3S"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuB3OGztds5-"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibtgbSlIds9a"
   },
   "outputs": [],
   "source": [
    "# beomi/KoAlpaca-KoRWKV-6B\n",
    "\n",
    "# 8비트\n",
    "# [ 3/1408 01:07 < 26:21:53, 0.01 it/s, Epoch 0.00/1]\n",
    "# 6973\n",
    "# 24793\n",
    "\n",
    "# 4비트\n",
    "# [ 7/1408 04:29 < 21:00:41, 0.02 it/s, Epoch 0.00/1]\n",
    "# 4325MiB\n",
    "# 18555MiB\n",
    "\n",
    "# 4비트 32 * 2\n",
    "# [ 3/352 03:16 < 19:04:18, 0.01 it/s, Epoch 0.01/1]\n",
    "# 4325MiB\n",
    "# 31663MiB\n",
    "\n",
    "# 4비트 48 * 2\n",
    "# [ 3/235 04:47 < 18:30:59, 0.00 it/s, Epoch 0.01/1]\n",
    "# 4325MiB\n",
    "# 39411MiB\n",
    "\n",
    "# beomi/KoAlpaca-KoRWKV-1.5B\n",
    "# 8비트\n",
    "# [ 3/352 01:42 < 9:57:31, 0.01 it/s, Epoch 0.01/1]\n",
    "# 1909MiB\n",
    "\n",
    "# 8비트 + 데이터 전처리\n",
    "# [ 3/1408 00:30 < 11:42:57, 0.03 it/s, Epoch 0.00/1]\n",
    "# 1909MiB\n",
    "# 13743MiB\n",
    "\n",
    "# 8비트 64*4\n",
    "# [ 3/88 06:19 < 8:57:03, 0.00 it/s, Epoch 0.02/1]\n",
    "# 1909MiB\n",
    "# 26421MiB\n",
    "\n",
    "\n",
    "# 4비트 32 * 2\n",
    "# [ 3/352 01:29 < 8:41:15, 0.01 it/s, Epoch 0.01/1]\n",
    "# 1357\n",
    "# 20191MiB"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNx3wdDWt9LbaeyXmJmJN0s",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
